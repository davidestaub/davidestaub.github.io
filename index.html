<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<title>Davide Staub - Personal Website</title>
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <header>
    <div class="header-content">
      <h1>Davide Staub</h1>
      <!-- Lab icon placed next to your name -->
      <img src="ssml-icon.webp" alt="Scalable Scientific Machine Learning Lab icon" class="lab-icon">
    </div>
  </header>
  <main>
    <section id="bio">
      <h2>Bio</h2>
      <div class="bio-content">
        <!-- Profile photo -->
        <img src="profile.jpg" alt="Profile photo" class="profile-pic">
        <p>
          I am Davide Staub, a PhD student at Imperial College London working in the
          <a href="https://scalable-sciml-lab.org/" target="_blank">Scalable Scientific Machine Learning Lab</a>
          under the supervision of Dr.&nbsp;Ben Moseley. My research focuses on
          harnessing machine learning and physics to build differentiable tools for
          reconstructing the three‑dimensional structure of exoplanet atmospheres from
          JWST observations.
          Before starting my PhD, I spent a year at the HomanLab within the
          Psychiatric University Hospital of Zürich, where we combined brain imaging
          and large language models to understand how the brain processes
          narratives.  
          <br>
          <a href="https://www.linkedin.com/in/davide-staub/" target="_blank">LinkedIn</a>
          <br>
          Email: <a href="mailto:dds25@ic.ac.uk">dds25@ic.ac.uk</a>
          <br>
          <!-- CV download link will work once CV_Davide_Staub.pdf is added to the repository -->
          <a href="CV_Davide_Staub.pdf" download>Download my CV</a>
        </p>
      </div>
    </section>

    <!-- Procedural exoplanet visualisation section -->
    <section id="exoplanet-viz">
      <h3>Exoplanet visualisation</h3>
      <!-- Container for the WebGL-rendered planet -->
      <div id="planet-container"></div>
    </section>
    <section id="teaching">
      <h2 onclick="toggleSection('teaching-content')" class="toggle">Teaching &#9654;</h2>
      <div id="teaching-content" class="content hidden">
        <p>
          <strong>Graduate Teaching Assistant, Imperial College London:</strong> I help deliver the MSc Deep Learning
          course at Imperial College London.  This involves running tutorials, answering students’ questions
          and supporting practical exercises on topics such as neural network fundamentals, optimisation
          and modern architectures.
        </p>
        <p>
          <strong>Lecturer, ETH Zürich – Space Data:</strong> I teach the final third of the
          <a href="https://eaps.ethz.ch/en/studies/master/space-systems.html" target="_blank">Space&nbsp;Data course</a>
          in the Master in Space Systems programme at ETH Zürich.  My block introduces convolutional neural
          networks and U‑Net architectures for denoising images of the Moon’s permanently shadowed regions (PSRs).
          These techniques build on the HORUS framework developed by Ben Moseley and Valentin Bickel, and are key to
          reliable resource mapping and landing‑site planning.  Students learn the basics of deep learning (MLPs,
          CNNs, U‑Nets, etc.) and then apply them to clean up PSR images using real training data.
        </p>
        <figure>
          <img src="psr_crater.png" alt="Image of a lunar crater with a dark permanently shadowed region used in teaching about lunar resource mapping" class="research-img">
          <figcaption>Permanently shadowed regions on the Moon, which require machine‑learning denoising for resource mapping and landing‑site planning.</figcaption>
        </figure>
      </div>
    </section>
    <section id="research">
      <h2 onclick="toggleSection('research-content')" class="toggle">Research &#9654;</h2>
      <div id="research-content" class="content hidden">
        <h3>Current research</h3>
        <p>
          My PhD project aims to develop a single, differentiable pipeline that
          converts all available JWST observations of a hot Jupiter—thermal
          emission spectra, phase curves, eclipses and transmission spectra—into a
          unified three‑dimensional temperature field \(T(\lambda, \phi, p)\). The
          map uses a low‑dimensional spherical‑harmonics basis for horizontal
          structure and smooth vertical modes, and the inversion is regularised
          using physically motivated terms such as energy balance, global
          radiative closure and hydrostatic consistency.  This
          framework treats the atmosphere as a shared state that must
          simultaneously explain observations across multiple viewing geometries.
        </p>

        <!-- Image illustrating current research on hot Jupiter exoplanets -->
        <div class="image-row">
          <figure>
            <img src="current_research.png" alt="Artist’s illustration of a hot Jupiter exoplanet orbiting close to its host star" class="research-img">
            <figcaption>Artist’s impression of a hot Jupiter exoplanet, showing how these giant planets orbit very close to their stars and face extreme atmospheric conditions.</figcaption>
          </figure>
        </div>
        <h3>Previous work</h3>
        <p>
          At the HomanLab in Zürich I explored how our brains follow the flow of stories.  Working with a
          language model, we distilled two simple signals: a <em>drift</em> signal that
          captures the gradual build‑up of meaning as a narrative unfolds, and a
          <em>shift</em> signal that spikes when the story moves to a new event or scene.
          When we compared these signals to high‑resolution fMRI recordings from a
          volunteer listening to crime stories, we found that the burst‑like
          shift signal lit up the brain’s speech and hearing centres, whereas the
          slow drift signal was strongest in the so‑called default‑mode network –
          regions like the angular gyrus, precuneus and posterior cingulate that
          support memory and imagination.  This pattern suggests that auditory
          areas mark event boundaries while broader networks follow the slow evolution
          of context.
        </p>
        <p>
          Below are brain maps showing where each signal explained neural
          responses.  Bright colours indicate regions with stronger effects.
        </p>
        <div class="image-row">
          <figure>
            <img src="shift_map.png" alt="Brain map showing robust shift responses in peri‑Sylvian regions" class="research-img">
            <figcaption>Shift: strong event‑boundary responses in auditory–language cortex.</figcaption>
          </figure>
          <figure>
            <img src="drift_map.png" alt="Brain map highlighting drift responses in higher‑order default‑mode regions" class="research-img">
            <figcaption>Drift: slow accumulation responses in higher‑order default‑mode regions.</figcaption>
          </figure>
        </div>
        <figure>
          <!-- Using the white‑background version of the method diagram to improve contrast on dark pages -->
          <img src="method_diagram_white.png" alt="Diagram summarising the model‑driven approach for deriving drift and shift signals from a language model and mapping them to brain activity" class="research-img">
          <figcaption>Method: converting stories into drift and shift signals via a large language model and mapping them to brain responses.</figcaption>
        </figure>
        <h3>Master’s thesis: Physics‑informed neural networks for seismology</h3>
        <p>
          During my master’s studies at ETH Zürich I investigated Physics‑Informed Neural
          Networks (PINNs) as a way to solve the elastic wave equation, which describes
          how seismic waves propagate through the Earth.  By embedding wave physics
          directly into the network architecture – for example using custom wavelet or
          plane‑wave layers along with encoder and decoder components – I achieved
          solutions that were roughly twice as accurate as standard PINNs.  I also conditioned the networks on the location of the seismic source, allowing them to infer the
          wavefield for countless source locations in a single forward pass and dramatically
          speeding up simulations compared with traditional finite‑difference methods.
        </p>
        <p>
          I presented this work at the 2024 British Seismology Meeting, and it was later summarised in
          <a href="https://academic.oup.com/astrogeo/article-abstract/66/3/3.29/8154308?redirectedFrom=fulltext" target="_blank"><em>Astronomy &amp; Geophysics</em></a>.  The report notes that incorporating wave physics improves
          accuracy and that once trained these networks are much faster than standard numerical solvers.  You can access the full thesis online via its DOI:
          <a href="https://doi.org/10.3929/ethz-b-000668359" target="_blank">10.3929/ethz‑b‑000668359</a>.
        </p>
        <div class="image-row">
          <figure>
            <img src="wavefield.png" alt="Simulation of radial wavefields propagating outward (red and blue rings) generated by physics-informed neural networks" class="research-img">
            <figcaption>Simulated elastic wavefields produced by a physics‑informed neural network.</figcaption>
          </figure>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <p>&copy; 2025 Davide Staub. All rights reserved.</p>
  </footer>
  <script>
    function toggleSection(id) {
      const section = document.getElementById(id);
      section.classList.toggle('hidden');
    }
  </script>

  <!-- Load Three.js library for WebGL rendering -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.159.0/build/three.min.js"></script>
  <!-- Procedural exoplanet rendering script -->
  <script>
    (function() {
      const container = document.getElementById('planet-container');
      // Only execute if the container exists and Three.js is available
      if (!container || typeof THREE === 'undefined') return;

      let width = container.clientWidth;
      let height = container.clientHeight || width * 0.6;

      const renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
      renderer.setSize(width, height);
      renderer.setPixelRatio(window.devicePixelRatio || 1);
      container.appendChild(renderer.domElement);

      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(35, width / height, 0.1, 100);
      // Position the camera a bit farther out to ensure the full planet fits in the square container
      camera.position.set(0, 0, 3.0);

      // Ambient and directional lights for realistic shading
      const ambient = new THREE.AmbientLight(0xffffff, 0.35);
      scene.add(ambient);
      const keyLight = new THREE.DirectionalLight(0xffffff, 1.1);
      keyLight.position.set(3, 2, 4);
      scene.add(keyLight);
      const rimLight = new THREE.DirectionalLight(0x88aaff, 0.5);
      rimLight.position.set(-3, -1, -4);
      scene.add(rimLight);

      // Helper to convert HSL to hex
      function hslToHex(h, s, l) {
        const a = s * Math.min(l, 1 - l);
        const f = n => {
          const k = (n + h * 12) % 12;
          const color = l - a * Math.max(Math.min(k - 3, 9 - k, 1), -1);
          return Math.round(255 * color);
        };
        return '#' + [f(0), f(8), f(4)].map(x => x.toString(16).padStart(2, '0')).join('');
      }

      // Create a procedural texture on a canvas
      function makePlanetTexture(size, baseColor, accentColor) {
        const canvas = document.createElement('canvas');
        canvas.width = canvas.height = size;
        const ctx = canvas.getContext('2d');

        /*
         * Generate a richly textured gas giant with swirling clouds, storms,
         * lightning and rain.  We paint directly into the pixel data for
         * maximum control.  The texture is horizontally tiled to give the
         * impression of banding and winds.
         */
        const imgData = ctx.createImageData(size, size);
        const data = imgData.data;

        // Convert hex colour strings to RGB arrays
        function hexToRgb(hex) {
          const bigint = parseInt(hex.replace('#', ''), 16);
          return [
            (bigint >> 16) & 255,
            (bigint >> 8) & 255,
            bigint & 255
          ];
        }

        const baseRGB = hexToRgb(baseColor);
        const accentRGB = hexToRgb(accentColor);

        // Parameters controlling features
        const swirlFreq = 3 + Math.random() * 3;      // number of swirling arms
        const swirlStrength = 0.5 + Math.random() * 0.5;
        const stormCount = 3 + Math.floor(Math.random() * 5);
        const lightningBolts = 2 + Math.floor(Math.random() * 3);
        const rainDensity = 0.0005 + Math.random() * 0.001;

        // Pre-generate storm centres
        const storms = [];
        for (let s = 0; s < stormCount; s++) {
          const cx = Math.random() * size;
          const cy = Math.random() * size;
          const radius = size * (0.02 + Math.random() * 0.05);
          storms.push({ cx, cy, radius });
        }

        // Pre-generate lightning bolts as random line segments
        const bolts = [];
        for (let b = 0; b < lightningBolts; b++) {
          const x0 = Math.random() * size;
          const y0 = Math.random() * size;
          const x1 = Math.random() * size;
          const y1 = Math.random() * size;
          bolts.push({ x0, y0, x1, y1 });
        }

        // For each pixel, compute patterns
        for (let y = 0; y < size; y++) {
          // Normalized vertical coordinate for rain bands
          const ny = y / size;
          for (let x = 0; x < size; x++) {
            const idx = (y * size + x) * 4;
            // Normalized coordinates in [-1,1]
            const nx = (x / size) * 2 - 1;
            const ny2 = (y / size) * 2 - 1;
            const r = Math.sqrt(nx * nx + ny2 * ny2);
            // Swirling pattern based on angle and radius
            let angle = Math.atan2(ny2, nx);
            let swirl = Math.sin((angle + r * swirlFreq) * Math.PI * swirlStrength);
             
            // Base colour blend between base and accent using swirl
            const mix = 0.4 + 0.5 * swirl;
            let rCol = baseRGB[0] * (1 - mix) + accentRGB[0] * mix;
            let gCol = baseRGB[1] * (1 - mix) + accentRGB[1] * mix;
            let bCol = baseRGB[2] * (1 - mix) + accentRGB[2] * mix;

            // Modulate brightness by radial falloff to hint at limb darkening
            const limb = 1 - Math.min(1, r * 0.7);
            rCol *= limb;
            gCol *= limb;
            bCol *= limb;

            // Add storms: brighten centre and adjust hue
            for (const st of storms) {
              const dx = x - st.cx;
              const dy = y - st.cy;
              const d2 = dx * dx + dy * dy;
              const rad2 = st.radius * st.radius;
              if (d2 < rad2) {
                const factor = 1 - d2 / rad2;
                rCol = rCol * (1 - factor) + accentRGB[0] * factor;
                gCol = gCol * (1 - factor) + accentRGB[1] * factor;
                bCol = bCol * (1 - factor) + accentRGB[2] * factor;
              }
            }

            // Add lightning: brighten pixels near random line segments
            for (const bolt of bolts) {
              // Distance from point to line segment
              const px = x;
              const py = y;
              const x0 = bolt.x0;
              const y0 = bolt.y0;
              const x1 = bolt.x1;
              const y1 = bolt.y1;
              const dx = x1 - x0;
              const dy = y1 - y0;
              const len2 = dx * dx + dy * dy;
              let t = ((px - x0) * dx + (py - y0) * dy) / len2;
              t = Math.max(0, Math.min(1, t));
              const projX = x0 + t * dx;
              const projY = y0 + t * dy;
              const dist = Math.hypot(px - projX, py - projY);
              if (dist < 2) {
                const spark = 1 - dist / 2;
                rCol += spark * 200;
                gCol += spark * 200;
                bCol += spark * 200;
              }
            }

            // Add rain as faint vertical streaks
            if (Math.random() < rainDensity) {
              const rainLength = 5 + Math.floor(Math.random() * 10);
              for (let ry = 0; ry < rainLength; ry++) {
                const ryi = y + ry;
                if (ryi >= size) break;
                const ridx = (ryi * size + x) * 4;
                data[ridx] = 220;
                data[ridx + 1] = 220;
                data[ridx + 2] = 255;
                data[ridx + 3] = 255;
              }
            }

            // Clamp values and assign to pixel data
            data[idx] = Math.min(255, Math.max(0, Math.round(rCol)));
            data[idx + 1] = Math.min(255, Math.max(0, Math.round(gCol)));
            data[idx + 2] = Math.min(255, Math.max(0, Math.round(bCol)));
            data[idx + 3] = 255;
          }
        }

        ctx.putImageData(imgData, 0, 0);
        const tex = new THREE.CanvasTexture(canvas);
        tex.wrapS = tex.wrapT = THREE.RepeatWrapping;
        tex.repeat.set(2, 1);
        tex.needsUpdate = true;
        return tex;
      }

      // Random palette selection for a hot Jupiter
      const baseHue = 0.02 + Math.random() * 0.08;
      const baseCol = hslToHex(baseHue, 0.7, 0.25);
      const accentCol = hslToHex(baseHue + 0.02, 0.9, 0.6);
      const planetTexture = makePlanetTexture(1024, baseCol, accentCol);

      // Create the planet mesh
      const planetGeo = new THREE.SphereGeometry(1, 96, 96);
      const planetMat = new THREE.MeshStandardMaterial({
        map: planetTexture,
        roughness: 0.65,
        metalness: 0.1
      });
      const planet = new THREE.Mesh(planetGeo, planetMat);
      scene.add(planet);

      // Thin cloud layer with its own rotation
      const cloudGeo = new THREE.SphereGeometry(1.015, 64, 64);
      const cloudTex = makePlanetTexture(512, '#ffffff', '#ffffff');
      cloudTex.offset.set(Math.random(), 0);
      const cloudMat = new THREE.MeshStandardMaterial({
        map: cloudTex,
        transparent: true,
        opacity: 0.35,
        depthWrite: false
      });
      const clouds = new THREE.Mesh(cloudGeo, cloudMat);
      scene.add(clouds);

      // Animate the planet and clouds
      function animate() {
        requestAnimationFrame(animate);
        planet.rotation.y += 0.002;
        clouds.rotation.y += 0.0027;
        planet.rotation.x = Math.sin(Date.now() * 0.0001) * 0.05;
        renderer.render(scene, camera);
      }
      animate();

      // Handle resizing
      window.addEventListener('resize', () => {
        width = container.clientWidth;
        height = container.clientHeight || width * 0.6;
        renderer.setSize(width, height);
        camera.aspect = width / height;
        camera.updateProjectionMatrix();
      });
    })();
  </script>
</body>
</html>
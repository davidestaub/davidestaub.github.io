<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<title>Davide Staub - Personal Website</title>
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <header>
    <div class="header-content">
      <h1>Davide Staub</h1>
      <!-- Lab icon placed next to your name -->
      <img src="ssml-icon.webp" alt="Scalable Scientific Machine Learning Lab icon" class="lab-icon">
    </div>
  </header>
  <main>
    <section id="bio">
      <h2>Bio</h2>
      <div class="bio-content">
        <!-- Profile photo -->
        <img src="profile.jpg" alt="Profile photo" class="profile-pic">
        <p>
          I am Davide Staub, a PhD student at Imperial College London working in the
          <a href="https://scalable-sciml-lab.org/" target="_blank">Scalable Scientific Machine Learning Lab</a>
          under the supervision of Dr.&nbsp;Ben Moseley. My research focuses on
          harnessing machine learning and physics to build differentiable tools for
          reconstructing the three‑dimensional structure of exoplanet atmospheres from
          JWST observations.
          Before starting my PhD, I spent a year at the HomanLab within the
          Psychiatric University Hospital of Zürich, where we combined brain imaging
          and large language models to understand how the brain processes
          narratives.  
          <br>
          <a href="https://www.linkedin.com/in/davide-staub/" target="_blank">LinkedIn</a>
          <br>
          Email: <a href="mailto:dds25@ic.ac.uk">dds25@ic.ac.uk</a>
          <br>
          <!-- CV download link will work once CV_Davide_Staub.pdf is added to the repository -->
          <a href="CV_Davide_Staub.pdf" download>Download my CV</a>
        </p>
      </div>
    </section>
    <section id="exoplanet-viz">
  <h3>Exoplanet visualisation</h3>
  <div id="planet-container"></div>
  <p class="viz-caption">
    Procedurally generated hot-Jupiter–style exoplanet. Refresh to see a new world.
  </p>
</section>

    <section id="teaching">
      <h2 onclick="toggleSection('teaching-content')" class="toggle">Teaching &#9654;</h2>
      <div id="teaching-content" class="content hidden">
        <p>
          <strong>Graduate Teaching Assistant, Imperial College London:</strong> I help deliver the MSc Deep Learning
          course at Imperial College London.  This involves running tutorials, answering students’ questions
          and supporting practical exercises on topics such as neural network fundamentals, optimisation
          and modern architectures.
        </p>
        <p>
          <strong>Lecturer, ETH Zürich – Space Data:</strong> I teach the final third of the
          <a href="https://eaps.ethz.ch/en/studies/master/space-systems.html" target="_blank">Space&nbsp;Data course</a>
          in the Master in Space Systems programme at ETH Zürich.  My block introduces convolutional neural
          networks and U‑Net architectures for denoising images of the Moon’s permanently shadowed regions (PSRs).
          These techniques build on the HORUS framework developed by Ben Moseley and Valentin Bickel, and are key to
          reliable resource mapping and landing‑site planning.  Students learn the basics of deep learning (MLPs,
          CNNs, U‑Nets, etc.) and then apply them to clean up PSR images using real training data.
        </p>
        <figure>
          <img src="psr_crater.png" alt="Image of a lunar crater with a dark permanently shadowed region used in teaching about lunar resource mapping" class="research-img">
          <figcaption>Permanently shadowed regions on the Moon, which require machine‑learning denoising for resource mapping and landing‑site planning.</figcaption>
        </figure>
      </div>
    </section>
    <section id="research">
      <h2 onclick="toggleSection('research-content')" class="toggle">Research &#9654;</h2>
      <div id="research-content" class="content hidden">
        <h3>Current research</h3>
        <p>
          My PhD project aims to develop a single, differentiable pipeline that
          converts all available JWST observations of a hot Jupiter-thermal
          emission spectra, phase curves, eclipses and transmission spectra-into a
          unified three‑dimensional temperature field. The
          map uses a low‑dimensional spherical‑harmonics basis for horizontal
          structure and smooth vertical modes, and the inversion is regularised
          using physically motivated terms such as energy balance, global
          radiative closure and hydrostatic consistency.  This
          framework treats the atmosphere as a shared state that must
          simultaneously explain observations across multiple viewing geometries.
        </p>

        <!-- Image illustrating current research on hot Jupiter exoplanets -->
        <div class="image-row">
          <figure>
            <img src="current_research.png" alt="Artist’s illustration of a hot Jupiter exoplanet orbiting close to its host star" class="research-img">
            <figcaption>Artist’s impression of a hot Jupiter exoplanet, showing how these giant planets orbit very close to their stars and face extreme atmospheric conditions.</figcaption>
          </figure>
        </div>
        <h3>Previous work</h3>
        <p>
          At the HomanLab in Zürich I explored how our brains follow the flow of stories.  Working with a
          language model, we distilled two simple signals: a <em>drift</em> signal that
          captures the gradual build‑up of meaning as a narrative unfolds, and a
          <em>shift</em> signal that spikes when the story moves to a new event or scene.
          When we compared these signals to high‑resolution fMRI recordings from a
          volunteer listening to crime stories, we found that the burst‑like
          shift signal lit up the brain’s speech and hearing centres, whereas the
          slow drift signal was strongest in the so‑called default‑mode network -
          regions like the angular gyrus, precuneus and posterior cingulate that
          support memory and imagination.  This pattern suggests that auditory
          areas mark event boundaries while broader networks follow the slow evolution
          of context.
        </p>
        <p>
          Below are brain maps showing where each signal explained neural
          responses.  Bright colours indicate regions with stronger effects.
        </p>
        <div class="image-row">
          <figure>
            <img src="shift_map.png" alt="Brain map showing robust shift responses in peri‑Sylvian regions" class="research-img">
            <figcaption>Shift: strong event‑boundary responses in auditory–language cortex.</figcaption>
          </figure>
          <figure>
            <img src="drift_map.png" alt="Brain map highlighting drift responses in higher‑order default‑mode regions" class="research-img">
            <figcaption>Drift: slow accumulation responses in higher‑order default‑mode regions.</figcaption>
          </figure>
        </div>
        <figure>
          <!-- Using the white‑background version of the method diagram to improve contrast on dark pages -->
          <img src="method_diagram_white.png" alt="Diagram summarising the model‑driven approach for deriving drift and shift signals from a language model and mapping them to brain activity" class="research-img">
          <figcaption>Method: converting stories into drift and shift signals via a large language model and mapping them to brain responses.</figcaption>
        </figure>
        <h3>Master’s thesis: Physics‑informed neural networks for seismology</h3>
        <p>
          During my master’s studies at ETH Zürich I investigated Physics‑Informed Neural
          Networks (PINNs) as a way to solve the elastic wave equation, which describes
          how seismic waves propagate through the Earth.  By embedding wave physics
          directly into the network architecture - for example using custom wavelet or
          plane‑wave layers along with encoder and decoder components - I achieved
          solutions that were roughly twice as accurate as standard PINNs.  I also conditioned the networks on the location of the seismic source, allowing them to infer the
          wavefield for countless source locations in a single forward pass and dramatically
          speeding up simulations compared with traditional finite‑difference methods.
        </p>
        <p>
          I presented this work at the 2024 British Seismology Meeting, and it was later summarised in
          <a href="https://academic.oup.com/astrogeo/article-abstract/66/3/3.29/8154308?redirectedFrom=fulltext" target="_blank"><em>Astronomy &amp; Geophysics</em></a>. The full thesis is accessible online via:
          <a href="https://doi.org/10.3929/ethz-b-000668359" target="_blank">10.3929/ethz‑b‑000668359</a>.
        </p>
        <div class="image-row">
          <figure>
            <img src="wavefield.png" alt="Simulation of radial wavefields propagating outward (red and blue rings) generated by physics-informed neural networks" class="research-img">
            <figcaption>Simulated elastic wavefields produced by a physics‑informed neural network.</figcaption>
          </figure>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <p>&copy; 2025 Davide Staub. All rights reserved.</p>
  </footer>
  <script>
    function toggleSection(id) {
      const section = document.getElementById(id);
      section.classList.toggle('hidden');
    }
  </script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.159.0/build/three.min.js"></script>
<script>
  (function() {
    const container = document.getElementById('planet-container');
    if (!container || !window.THREE) return;

    const width  = container.clientWidth;
    const height = container.clientHeight || width * 0.6;

    const renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
    renderer.setSize(width, height);
    renderer.setPixelRatio(window.devicePixelRatio || 1);
    container.appendChild(renderer.domElement);

    const scene = new THREE.Scene();

    const camera = new THREE.PerspectiveCamera(35, width / height, 0.1, 100);
    camera.position.set(0, 0, 3.2);

    // Lights
    const ambient = new THREE.AmbientLight(0xffffff, 0.35);
    scene.add(ambient);

    const keyLight = new THREE.DirectionalLight(0xffffff, 1.1);
    keyLight.position.set(3, 2, 4);
    scene.add(keyLight);

    const rimLight = new THREE.DirectionalLight(0x88aaff, 0.5);
    rimLight.position.set(-3, -1, -4);
    scene.add(rimLight);

    // Helper: create a procedural texture in a <canvas>
    function makePlanetTexture(size, baseColor, accentColor) {
      const canvas = document.createElement('canvas');
      canvas.width = canvas.height = size;
      const ctx = canvas.getContext('2d');

      // radial gradient for day/night + hotspots
      const grad = ctx.createRadialGradient(
        size * 0.25, size * 0.3, size * 0.1,
        size * 0.55, size * 0.6, size * 0.7
      );
      grad.addColorStop(0, accentColor);
      grad.addColorStop(0.4, baseColor);
      grad.addColorStop(1, '#050509');
      ctx.fillStyle = grad;
      ctx.fillRect(0, 0, size, size);

      // add noisy bands (winds / clouds / chemistry)
      const img = ctx.getImageData(0, 0, size, size);
      const data = img.data;
      const bandCount = 12 + Math.floor(Math.random() * 8);

      for (let y = 0; y < size; y++) {
        const band = Math.floor((y / size) * bandCount);
        const bandPhase = band / bandCount;
        for (let x = 0; x < size; x++) {
          const i = (y * size + x) * 4;
          const noise =
            (Math.random() - 0.5) * 25 +
            Math.sin((x / size) * Math.PI * 4 + bandPhase * Math.PI * 2) * 18;

          for (let c = 0; c < 3; c++) {
            let v = data[i + c] + noise;
            data[i + c] = v < 0 ? 0 : v > 255 ? 255 : v;
          }

          // occasional bright storms
          if (Math.random() < 0.002) {
            data[i]     = 255;
            data[i + 1] = 240;
            data[i + 2] = 200;
          }
        }
      }
      ctx.putImageData(img, 0, 0);

      const tex = new THREE.CanvasTexture(canvas);
      tex.wrapS = tex.wrapT = THREE.RepeatWrapping;
      tex.repeat.set(2, 1);
      tex.needsUpdate = true;
      return tex;
    }

    function hex(r, g, b) {
      return '#' + [r, g, b].map(v => v.toString(16).padStart(2, '0')).join('');
    }

    // Random hot-Jupiter palette
    const baseHue = 0.02 + Math.random() * 0.08; // orange / red range
    const toRGB = (h, s, l) => {
      // tiny HSL -> RGB helper
      const f = (n, k = (n + h * 12) % 12) =>
        l - s * Math.max(Math.min(k - 3, 9 - k, 1), -1);
      const r = Math.round(f(0) * 255);
      const g = Math.round(f(8) * 255);
      const b = Math.round(f(4) * 255);
      return hex(r, g, b);
    };
    const baseCol   = toRGB(baseHue, 0.7, 0.25);
    const accentCol = toRGB(baseHue + 0.02, 0.9, 0.6);

    const planetTex = makePlanetTexture(1024, baseCol, accentCol);

    // Main planet
    const planetGeo = new THREE.SphereGeometry(1, 96, 96);
    const planetMat = new THREE.MeshStandardMaterial({
      map: planetTex,
      roughness: 0.65,
      metalness: 0.1
    });
    const planet = new THREE.Mesh(planetGeo, planetMat);
    scene.add(planet);

    // Thin, semi-transparent cloud layer that moves at a different speed
    const cloudsGeo = new THREE.SphereGeometry(1.015, 64, 64);
    const cloudsTex = makePlanetTexture(512, '#ffffff', '#ffffff');
    cloudsTex.offset.set(Math.random(), 0);
    const cloudsMat = new THREE.MeshStandardMaterial({
      map: cloudsTex,
      transparent: true,
      opacity: 0.35,
      depthWrite: false
    });
    const clouds = new THREE.Mesh(cloudsGeo, cloudsMat);
    scene.add(clouds);

    // Animation loop
    function animate() {
      requestAnimationFrame(animate);
      planet.rotation.y += 0.002;
      clouds.rotation.y += 0.0027;
      // tiny rocking for a more “alive” feel
      planet.rotation.x = Math.sin(Date.now() * 0.0001) * 0.05;
      renderer.render(scene, camera);
    }
    animate();

    // Handle resize
    window.addEventListener('resize', () => {
      const w = container.clientWidth;
      const h = container.clientHeight || w * 0.6;
      renderer.setSize(w, h);
      camera.aspect = w / h;
      camera.updateProjectionMatrix();
    });
  })();
</script>

</body>
</html>

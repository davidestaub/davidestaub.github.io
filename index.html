<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<title>Davide Staub - Personal Website</title>
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <header>
    <div class="header-content">
      <h1>Davide Staub</h1>
      <!-- Lab icon placed next to your name -->
      <img src="ssml-icon.webp" alt="Scalable Scientific Machine Learning Lab icon" class="lab-icon">
    </div>
  </header>
  <main>
    <section id="bio">
      <h2>Bio</h2>
      <div class="bio-content">
        <!-- Profile photo -->
        <img src="profile.jpg" alt="Profile photo" class="profile-pic">
        <!-- Exoplanet visualisation container placed next to the profile photo.  This
             will render the rotating planet with a satellite orbit in the top
             right corner of the bio section. -->
        <div id="planet-container"></div>
        <p>
          I am Davide Staub, a PhD student at Imperial College London working in the
          <a href="https://scalable-sciml-lab.org/" target="_blank">Scalable Scientific Machine Learning Lab</a>
          under the supervision of Dr.&nbsp;Ben Moseley. My research focuses on
          harnessing machine learning and physics to build differentiable tools for
          reconstructing the three‑dimensional structure of exoplanet atmospheres from
          JWST observations.
          Before starting my PhD, I spent a year at the HomanLab within the
          Psychiatric University Hospital of Zürich, where we combined brain imaging
          and large language models to understand how the brain processes
          narratives.  
          <br>
          <a href="https://www.linkedin.com/in/davide-staub/" target="_blank">LinkedIn</a>
          <br>
          Email: <a href="mailto:dds25@ic.ac.uk">dds25@ic.ac.uk</a>
          <br>
          <!-- CV download link will work once CV_Davide_Staub.pdf is added to the repository -->
          <a href="CV_Davide_Staub.pdf" download>Download my CV</a>
        </p>
      </div>
    </section>

    <!-- Interactive exoplanet research game section.  This replaces the old
         exoplanet visualisation heading and sits below the bio.  The game
         challenges visitors to detect transits (dips in a star’s light curve)
         by clicking when the brightness drops. -->
    <section id="exoplanet-game">
      <h3>Transit detection game</h3>
      <p class="game-instructions">Click the graph when you see the star dim. Each correct detection earns a point!</p>
      <canvas id="transit-canvas" width="600" height="150"></canvas>
      <p id="game-score">Score: 0</p>
      <p id="game-message"></p>
    </section>
    <section id="teaching">
      <h2 onclick="toggleSection('teaching-content')" class="toggle">Teaching &#9654;</h2>
      <div id="teaching-content" class="content hidden">
        <p>
          <strong>Graduate Teaching Assistant, Imperial College London:</strong> I help deliver the MSc Deep Learning
          course at Imperial College London.  This involves running tutorials, answering students’ questions
          and supporting practical exercises on topics such as neural network fundamentals, optimisation
          and modern architectures.
        </p>
        <p>
          <strong>Lecturer, ETH Zürich – Space Data:</strong> I teach the final third of the
          <a href="https://eaps.ethz.ch/en/studies/master/space-systems.html" target="_blank">Space&nbsp;Data course</a>
          in the Master in Space Systems programme at ETH Zürich.  My block introduces convolutional neural
          networks and U‑Net architectures for denoising images of the Moon’s permanently shadowed regions (PSRs).
          These techniques build on the HORUS framework developed by Ben Moseley and Valentin Bickel, and are key to
          reliable resource mapping and landing‑site planning.  Students learn the basics of deep learning (MLPs,
          CNNs, U‑Nets, etc.) and then apply them to clean up PSR images using real training data.
        </p>
        <figure>
          <img src="psr_crater.png" alt="Image of a lunar crater with a dark permanently shadowed region used in teaching about lunar resource mapping" class="research-img">
          <figcaption>Permanently shadowed regions on the Moon, which require machine‑learning denoising for resource mapping and landing‑site planning.</figcaption>
        </figure>
      </div>
    </section>
    <section id="research">
      <h2 onclick="toggleSection('research-content')" class="toggle">Research &#9654;</h2>
      <div id="research-content" class="content hidden">
        <h3>Current research</h3>
        <p>
          My PhD project aims to develop a single, differentiable pipeline that
          converts all available JWST observations of a hot Jupiter—thermal
          emission spectra, phase curves, eclipses and transmission spectra—into a
          unified three‑dimensional temperature field \(T(\lambda, \phi, p)\). The
          map uses a low‑dimensional spherical‑harmonics basis for horizontal
          structure and smooth vertical modes, and the inversion is regularised
          using physically motivated terms such as energy balance, global
          radiative closure and hydrostatic consistency.  This
          framework treats the atmosphere as a shared state that must
          simultaneously explain observations across multiple viewing geometries.
        </p>

        <!-- Image illustrating current research on hot Jupiter exoplanets -->
        <div class="image-row">
          <figure>
            <img src="current_research.png" alt="Artist’s illustration of a hot Jupiter exoplanet orbiting close to its host star" class="research-img">
            <figcaption>Artist’s impression of a hot Jupiter exoplanet, showing how these giant planets orbit very close to their stars and face extreme atmospheric conditions.</figcaption>
          </figure>
        </div>
        <h3>Previous work</h3>
        <p>
          At the HomanLab in Zürich I explored how our brains follow the flow of stories.  Working with a
          language model, we distilled two simple signals: a <em>drift</em> signal that
          captures the gradual build‑up of meaning as a narrative unfolds, and a
          <em>shift</em> signal that spikes when the story moves to a new event or scene.
          When we compared these signals to high‑resolution fMRI recordings from a
          volunteer listening to crime stories, we found that the burst‑like
          shift signal lit up the brain’s speech and hearing centres, whereas the
          slow drift signal was strongest in the so‑called default‑mode network –
          regions like the angular gyrus, precuneus and posterior cingulate that
          support memory and imagination.  This pattern suggests that auditory
          areas mark event boundaries while broader networks follow the slow evolution
          of context.
        </p>
        <p>
          Below are brain maps showing where each signal explained neural
          responses.  Bright colours indicate regions with stronger effects.
        </p>
        <div class="image-row">
          <figure>
            <img src="shift_map.png" alt="Brain map showing robust shift responses in peri‑Sylvian regions" class="research-img">
            <figcaption>Shift: strong event‑boundary responses in auditory–language cortex.</figcaption>
          </figure>
          <figure>
            <img src="drift_map.png" alt="Brain map highlighting drift responses in higher‑order default‑mode regions" class="research-img">
            <figcaption>Drift: slow accumulation responses in higher‑order default‑mode regions.</figcaption>
          </figure>
        </div>
        <figure>
          <!-- Using the white‑background version of the method diagram to improve contrast on dark pages -->
          <img src="method_diagram_white.png" alt="Diagram summarising the model‑driven approach for deriving drift and shift signals from a language model and mapping them to brain activity" class="research-img">
          <figcaption>Method: converting stories into drift and shift signals via a large language model and mapping them to brain responses.</figcaption>
        </figure>
        <h3>Master’s thesis: Physics‑informed neural networks for seismology</h3>
        <p>
          During my master’s studies at ETH Zürich I investigated Physics‑Informed Neural
          Networks (PINNs) as a way to solve the elastic wave equation, which describes
          how seismic waves propagate through the Earth.  By embedding wave physics
          directly into the network architecture – for example using custom wavelet or
          plane‑wave layers along with encoder and decoder components – I achieved
          solutions that were roughly twice as accurate as standard PINNs.  I also conditioned the networks on the location of the seismic source, allowing them to infer the
          wavefield for countless source locations in a single forward pass and dramatically
          speeding up simulations compared with traditional finite‑difference methods.
        </p>
        <p>
          I presented this work at the 2024 British Seismology Meeting, and it was later summarised in
          <a href="https://academic.oup.com/astrogeo/article-abstract/66/3/3.29/8154308?redirectedFrom=fulltext" target="_blank"><em>Astronomy &amp; Geophysics</em></a>.  The report notes that incorporating wave physics improves
          accuracy and that once trained these networks are much faster than standard numerical solvers.  You can access the full thesis online via its DOI:
          <a href="https://doi.org/10.3929/ethz-b-000668359" target="_blank">10.3929/ethz‑b‑000668359</a>.
        </p>
        <div class="image-row">
          <figure>
            <img src="wavefield.png" alt="Simulation of radial wavefields propagating outward (red and blue rings) generated by physics-informed neural networks" class="research-img">
            <figcaption>Simulated elastic wavefields produced by a physics‑informed neural network.</figcaption>
          </figure>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <p>&copy; 2025 Davide Staub. All rights reserved.</p>
  </footer>
  <script>
    function toggleSection(id) {
      const section = document.getElementById(id);
      section.classList.toggle('hidden');
    }
  </script>

  <!-- Interactive transit detection game script -->
  <script>
    (function() {
      const canvas = document.getElementById('transit-canvas');
      if (!canvas) return;
      const ctx = canvas.getContext('2d');
      let width = canvas.width;
      let height = canvas.height;

      // Simulation time and transit schedule
      let time = 0;
      // Pre-generate a series of transit start times.  These represent when
      // the exoplanet passes in front of its star.  After a correct detection,
      // transits are removed from the list.
      let transitTimes = generateTransitTimes();
      function generateTransitTimes() {
        const times = [];
        let t = 2 + Math.random() * 2;
        for (let i = 0; i < 6; i++) {
          times.push(t);
          t += 3 + Math.random() * 3;
        }
        return times;
      }

      // Score and message elements
      const scoreEl = document.getElementById('game-score');
      const messageEl = document.getElementById('game-message');
      let score = 0;

      // Render the light curve.  The brightness fluctuates gently and dips
      // significantly during transits.  A simple mapping converts brightness
      // values to vertical positions on the canvas.
      function draw() {
        ctx.fillStyle = '#111';
        ctx.fillRect(0, 0, width, height);
        ctx.beginPath();
        const maxTimeWindow = 10; // seconds represented across the canvas width
        const sampleCount = width;
        for (let i = 0; i < sampleCount; i++) {
          // Determine the time corresponding to this pixel
          const tSample = time - (sampleCount - i) / sampleCount * maxTimeWindow;
          // Baseline brightness
          let brightness = 1.0;
          // Small random noise to simulate measurement variability
          brightness += (Math.random() - 0.5) * 0.02;
          // Apply transit dips using a cosine-shaped drop for smoother curves
          for (const ts of transitTimes) {
            // Each transit lasts 0.5 seconds
            const duration = 0.5;
            if (tSample >= ts && tSample <= ts + duration) {
              const phase = (tSample - ts) / duration; // 0 to 1
              brightness -= 0.25 * Math.cos(Math.PI * phase);
            }
          }
          // Map brightness (range ~0.7–1.3) to vertical position (0 at top)
          const y = height - ((brightness - 0.6) / 0.8) * height;
          if (i === 0) ctx.moveTo(i, y);
          else ctx.lineTo(i, y);
        }
        ctx.strokeStyle = '#00bfff';
        ctx.lineWidth = 2;
        ctx.stroke();
      }

      function update() {
        time += 0.02;
        draw();
        requestAnimationFrame(update);
      }
      update();

      // Click handler: detect if a transit is occurring when the user clicks
      canvas.addEventListener('click', () => {
        // Find a transit that is currently in progress (within +/-0.3s)
        let hit = false;
        for (let i = 0; i < transitTimes.length; i++) {
          const ts = transitTimes[i];
          if (Math.abs(time - (ts + 0.25)) < 0.3) {
            // Correct detection
            hit = true;
            score++;
            scoreEl.textContent = 'Score: ' + score;
            messageEl.textContent = 'Good catch!';
            // Remove this transit so it cannot be detected twice
            transitTimes.splice(i, 1);
            break;
          }
        }
        if (!hit) {
          messageEl.textContent = 'Miss – try again!';
        }
        // If all transits are detected, generate a new schedule
        if (transitTimes.length === 0) {
          transitTimes = generateTransitTimes();
          messageEl.textContent = 'New transits generated!';
        }
      });
    })();
  </script>

  <!-- Load Three.js library for WebGL rendering -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.159.0/build/three.min.js"></script>
  <!-- Procedural exoplanet rendering script -->
  <script>
    (function() {
      const container = document.getElementById('planet-container');
      // Only execute if the container exists and Three.js is available
      if (!container || typeof THREE === 'undefined') return;

      let width = container.clientWidth;
      // Use the container's computed height when available; otherwise fall back to a square
      let height = container.clientHeight || width;

      const renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
      renderer.setSize(width, height);
      renderer.setPixelRatio(window.devicePixelRatio || 1);
      container.appendChild(renderer.domElement);

      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(35, width / height, 0.1, 100);
      // Position the camera a bit farther out to ensure the full planet fits in the square container
      camera.position.set(0, 0, 3.4);

      // Lighting setup: use a slightly brighter ambient light and a single key
      // light.  Removing the rim light prevents an outer halo and reduces
      // reflections that can be mistaken for a secondary hull.  Increasing
      // ambient intensity softens the shading for a more cohesive look.
      const ambient = new THREE.AmbientLight(0xffffff, 0.45);
      scene.add(ambient);
      const keyLight = new THREE.DirectionalLight(0xffffff, 0.9);
      keyLight.position.set(3, 2, 4);
      scene.add(keyLight);

      // Helper to convert HSL to hex
      function hslToHex(h, s, l) {
        const a = s * Math.min(l, 1 - l);
        const f = n => {
          const k = (n + h * 12) % 12;
          const color = l - a * Math.max(Math.min(k - 3, 9 - k, 1), -1);
          return Math.round(255 * color);
        };
        return '#' + [f(0), f(8), f(4)].map(x => x.toString(16).padStart(2, '0')).join('');
      }

      // Create a procedural texture on a canvas
      function makePlanetTexture(size, baseColor, accentColor) {
        const canvas = document.createElement('canvas');
        canvas.width = canvas.height = size;
        const ctx = canvas.getContext('2d');

        /*
         * Generate a gas giant texture with convincing swirling clouds and storms.
         * We combine multiple sine waves to create turbulent bands and then
         * sprinkle storms for variety.  A small amount of noise breaks up
         * uniform regions.  The texture is horizontally tiled to simulate
         * planetary rotation.
         */
        const imgData = ctx.createImageData(size, size);
        const data = imgData.data;

        // Convert hex colour strings to RGB arrays
        function hexToRgb(hex) {
          const bigint = parseInt(hex.replace('#', ''), 16);
          return [
            (bigint >> 16) & 255,
            (bigint >> 8) & 255,
            bigint & 255
          ];
        }

        const baseRGB = hexToRgb(baseColor);
        const accentRGB = hexToRgb(accentColor);

        // Frequencies and phases for sine wave patterns
        const freqX = 4 + Math.random() * 4;
        const freqY = 3 + Math.random() * 3;
        const freqDiag = 5 + Math.random() * 3;
        const phaseX = Math.random() * Math.PI * 2;
        const phaseY = Math.random() * Math.PI * 2;
        const phaseDiag = Math.random() * Math.PI * 2;
        const swirlStrength = 2 + Math.random() * 2;

        // Storms
        const stormCount = 2 + Math.floor(Math.random() * 4);
        const storms = [];
        for (let s = 0; s < stormCount; s++) {
          const cx = Math.random() * size;
          const cy = Math.random() * size;
          const radius = size * (0.04 + Math.random() * 0.06);
          storms.push({ cx, cy, radius });
        }

        for (let y = 0; y < size; y++) {
          const ny = y / size;
          for (let x = 0; x < size; x++) {
            const nx = x / size;
            const idx = (y * size + x) * 4;
            // Sine-based turbulent pattern
            const v =
              Math.sin(nx * freqX * Math.PI * 2 + phaseX) +
              Math.sin(ny * freqY * Math.PI * 2 + phaseY) +
              Math.sin((nx + ny) * freqDiag * Math.PI * 2 + phaseDiag);
            // Normalise v to 0..1
            const mix = 0.5 + 0.5 * Math.sin(v * swirlStrength);
            let rCol = baseRGB[0] * (1 - mix) + accentRGB[0] * mix;
            let gCol = baseRGB[1] * (1 - mix) + accentRGB[1] * mix;
            let bCol = baseRGB[2] * (1 - mix) + accentRGB[2] * mix;

            // Limb darkening: fade colours near the edge
            const cxNorm = (nx - 0.5) * 2;
            const cyNorm = (ny - 0.5) * 2;
            const dist = Math.sqrt(cxNorm * cxNorm + cyNorm * cyNorm);
            const limb = 1 - Math.min(1, dist * 0.8);
            rCol *= limb;
            gCol *= limb;
            bCol *= limb;

            // Add storms: brighten towards the centre with accent colours
            for (const st of storms) {
              const dx = x - st.cx;
              const dy = y - st.cy;
              const d2 = dx * dx + dy * dy;
              const rad2 = st.radius * st.radius;
              if (d2 < rad2) {
                const factor = 1 - d2 / rad2;
                rCol = rCol * (1 - factor) + accentRGB[0] * factor;
                gCol = gCol * (1 - factor) + accentRGB[1] * factor;
                bCol = bCol * (1 - factor) + accentRGB[2] * factor;
              }
            }

            // Add moderate random noise for realism
            const noise = (Math.random() - 0.5) * 30;
            rCol += noise;
            gCol += noise;
            bCol += noise;

            // Clamp values
            data[idx] = Math.min(255, Math.max(0, Math.round(rCol)));
            data[idx + 1] = Math.min(255, Math.max(0, Math.round(gCol)));
            data[idx + 2] = Math.min(255, Math.max(0, Math.round(bCol)));
            data[idx + 3] = 255;
          }
        }

        ctx.putImageData(imgData, 0, 0);
        const tex = new THREE.CanvasTexture(canvas);
        tex.wrapS = tex.wrapT = THREE.RepeatWrapping;
        tex.repeat.set(2, 1);
        tex.needsUpdate = true;
        return tex;
      }

      // Prepare an array of pre‑generated planetary textures.  Each image is a
      // colour‑shifted version of Jupiter's cloud bands.  Choosing randomly
      // from this list on each page load makes the exoplanet look different
      // every time while maintaining realistic structure.
      const textureNames = [
        'exoplanet_var1.jpg',
        'exoplanet_var2.jpg',
        'exoplanet_var3.jpg',
        'exoplanet_var4.jpg',
        'exoplanet_var5.jpg'
      ];
      const textureLoader = new THREE.TextureLoader();
      const chosenName = textureNames[Math.floor(Math.random() * textureNames.length)];
      const baseTexture = textureLoader.load(chosenName);
      // Repeat horizontally and clamp vertically to avoid seams
      baseTexture.wrapS = THREE.RepeatWrapping;
      baseTexture.wrapT = THREE.ClampToEdgeWrapping;
      // Randomise the longitudinal offset so the great red spot appears at
      // different positions on each load
      baseTexture.offset.set(Math.random(), 0);

      // Choose a random tint colour by sampling the entire HSL hue range.  A
      // modest saturation and mid‑level lightness ensure the banded structure
      // remains visible while giving the planet a unique hue.
      const tintHue = Math.random();
      const tintColor = new THREE.Color();
      tintColor.setHSL(tintHue, 0.4 + Math.random() * 0.2, 0.5 + Math.random() * 0.2);

      // Create the planet mesh.  Reduce the radius to 0.5 to further decrease
      // the planet's apparent size.  Apply the chosen texture and tint.
      // Reduce the sphere radius further so the planet comfortably fits
      // within the now smaller container.  A radius of 0.4 provides a
      // roughly three‑fold reduction in apparent size compared with the
      // original version.
      // Create a slightly larger planet mesh.  A radius of 0.45 makes the
      // sphere more prominent while still fitting within the container.  A
      // completely matte material avoids specular highlights.
      // Slightly enlarge the planet radius so it appears bigger relative to
      // the container.  Increasing from 0.45 to 0.5 preserves the
      // three-dimensional feel while keeping the globe fully in view.
      const planetGeo = new THREE.SphereGeometry(0.5, 96, 96);
      const planetMat = new THREE.MeshStandardMaterial({
        map: baseTexture,
        color: tintColor,
        roughness: 1.0,
        metalness: 0.0
      });
      const planet = new THREE.Mesh(planetGeo, planetMat);
      scene.add(planet);

      // Add a small satellite orbiting the planet.  This sphere will move
      // around the planet on a circular orbit.  Clicking the planet area
      // randomises the orbit radius, tilt and speed.
      const satelliteGeo = new THREE.SphereGeometry(0.07, 24, 24);
      const satelliteMat = new THREE.MeshStandardMaterial({ color: 0xffffff, roughness: 1.0, metalness: 0.0 });
      const satellite = new THREE.Mesh(satelliteGeo, satelliteMat);
      scene.add(satellite);

      // Orbit parameters
      let orbitRadius = 0.9;
      let orbitSpeed = 0.005;
      let orbitTilt = Math.PI / 8;
      let orbitPhase = 0;

      // Function to randomise the orbit when the user clicks the container
      container.addEventListener('click', () => {
        orbitRadius = 0.6 + Math.random() * 0.4; // range 0.6–1.0
        orbitSpeed = 0.003 + Math.random() * 0.007;
        orbitTilt = Math.random() * Math.PI / 3; // up to 60° tilt
      });

      // Animation loop: rotate the planet and update the satellite's position
      function animate() {
        requestAnimationFrame(animate);
        planet.rotation.y += 0.002;
        planet.rotation.x = Math.sin(Date.now() * 0.0001) * 0.05;
        orbitPhase += orbitSpeed;
        // Compute satellite orbit in tilted plane
        const x = orbitRadius * Math.cos(orbitPhase);
        const z = orbitRadius * Math.sin(orbitPhase);
        const y = orbitRadius * Math.sin(orbitPhase) * Math.sin(orbitTilt);
        // Apply a rotation matrix for tilt
        satellite.position.set(
          x * Math.cos(orbitTilt) - z * Math.sin(orbitTilt),
          y,
          x * Math.sin(orbitTilt) + z * Math.cos(orbitTilt)
        );
        renderer.render(scene, camera);
      }
      animate();

      // Handle resizing of the container
      window.addEventListener('resize', () => {
        width = container.clientWidth;
        height = container.clientHeight || width;
        renderer.setSize(width, height);
        camera.aspect = width / height;
        camera.updateProjectionMatrix();
      });
    })();
  </script>
</body>
</html>